---
title: "WebXR Depth Sensing Moduleについての調査"
emoji: "🕵️‍♂️"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["webar", "ar", "javascript", "typescript"]
published: false
---

:::message
本記事は[WebXR アドベントカレンダー](https://adventar.org/calendars/7401)のｎ日目の記事です。
:::

# はじめに

## TL;DR

## 記事の概要と対象読者

本記事では WebXR Depth Sensing Module というものについて解説します。
詳細は後述しますが、ブラウザに実装されている WebAR の API なので WebAR 開発者向けの内容になります。

また本記事では JavaScript・TypeScript・Babylon.js などの技術スタックを説明なしに取り上げますので、それらの知識があると望ましいです。

## 検証環境

筆者が検証に用いた環境を次に示します。

| 環境          | バージョンなど                |
| :------------ | :---------------------------- |
| 開発機        | Windows 10 Home               |
| デバッグ機    | Android 12 Google Pixel 4a 5G |
| Google Chrome | Chrome for Android 106        |
| Node.js       | 16.13.0                       |
| yarn          | 1.22.18                       |
| Vite          | 3.1.0                         |
| TypeScript    | 4                             |
| Babylon.js    | 5.27.0                        |

## サンプルプロジェクト

WebXR Depth Sensing Module を Babylon.js で使ったサンプルプロジェクトを GitHub に公開しておりますので、合わせてご覧ください。

https://github.com/drumath2237/webxr-depth-testbed-babylon

# WebXR Depth Sensing Module の概要

## このModuleはいったい何なのか

WebXR Depth Sensing Module とは、WebXR Device API で提供されている Module の一種です。
W3C の Working Draft に概要や使い方などが記されています。

https://www.w3.org/TR/2022/WD-webxr-depth-sensing-1-20220419/

草案の GitHub リポジトリは以下です。

https://github.com/immersive-web/depth-sensing/

## 何ができるのか

この Module は名前の通り、Depth、つまり深度情報を取得するための API を提供します。
深度情報とは WebAR デバイスのカメラ画角において、「ここらへんに見えている物はどのくらいユーザから離れているのか」を表すものです。

例えば iPhone には LiDAR センサーが載っている機種がありますが、LiDAR センサーを使うことで光が飛んでいる時間から光が到達した物体までの距離を割り出すことができます。また LiDAR センサーが載っていない ARCore に対応した Android デバイスも、画像処理によって深度推定をして距離を求められます。
今紹介したものはいずれもネイティブアプリの API によって実現可能ですが、これらをブラウザからでも実現したものが WebXR Depth Sensing Module です。
Depth 画像を使うと物体の形状を 3 次元的に復元したり、AR シーンでオクルージョンを実現したり、空間認識の精度を向上させたりできます。

![img](https://docs-assets.developer.apple.com/published/e2a121be00/rendered2x-1611871073.png)
*Depth画像の参考（引用：[Apple Developer](https://developer.apple.com/documentation/arkit/environmental_analysis/displaying_a_point_cloud_using_scene_depth)）*

## どうやって使うのか

使い方を知るには GitHub の explaner を見るのが一番簡単です。
基本的な使い方と型定義などが書いてあります。もっと詳しく知りたい場合は W3C の Working Draft を見てみましょう。

https://github.com/immersive-web/depth-sensing/blob/main/explainer.md

### 機能の有効化（リクエスト）

explaner に書いてある通り、WebXRSession を受け取る際にオプションとして DepthSesning の機能をリクエストします。

```js
const session = await navigator.xr.requestSession("immersive-ar", {
  requiredFeatures: ["depth-sensing"],
  depthSensing: {
    usagePreference: ["cpu-optimized", "gpu-optimized"],
    formatPreference: ["luminance-alpha", "float32"]
  }
});
```

ここで特徴的なのは「Usage」と「Format」を指定できる点でしょう。

### UsageとFormat

### CPUモードで深度情報を取得する

### GPUモードで深度情報を取得する

## 動作要件など

https://immersiveweb.dev/#supporttable

# Babylon.js での扱い方（2022 年 12 月時点）

## `scene.createDefaultXRExperienceAsync`では使えない

## `enterXRAsync`を呼び出して解決

# おわりに

## 参考文献

https://www.w3.org/TR/2022/WD-webxr-depth-sensing-1-20220419/

https://github.com/immersive-web/depth-sensing/

https://zenn.dev/drumath2237/scraps/9df1e32f7de989
